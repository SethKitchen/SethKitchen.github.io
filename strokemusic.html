<html>

<body class="is-preload">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/mainOther.css" />
    <noscript>
        <link rel="stylesheet" href="assets/css/noscript.css" />
    </noscript>
    <script src=" assets/js/wavesurfer.regions.min.js"> </script> <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/FileSaver.min.js"></script>
    <div id="loading" style="display:none">
        <div id="animation" style="display: flex; justify-content: center;">
            <img src='images/loading.gif' />
        </div>
        <span style="display: flex; justify-content: center;">Downloading "MyCreation.wav" to downloads folder...</span>
    </div>
    <p>
        Most Likely Tempo (bpm): <span style="width:200px;" id="tempo"> </span>
    </p>

    <button onclick="startAudio()">Play Song </button>

    <script>
        var canvas = null;
        var audioCtx = null;
        var recognition = null;
        var song = null;
        var userRecorded = null;
        var segmentEnd = null;

        window.onload = function () {
            window.SpeechRecognition = window.webkitSpeechRecognition || window.SpeechRecognition;
            if ('SpeechRecognition' in window) {
                // speech recognition API supported
                while (!audioCtx) {
                    var AudioContext = window.AudioContext || window.webkitAudioContext;
                    audioCtx = new AudioContext();
                }
                recognition = new window.SpeechRecognition();
                recognition.interimResults = true;
                recognition.onresult = receivedRecognition;
                song = new Audio('sunshine.mp3');
                var source = audioCtx.createMediaElementSource(song);
                finalBuffer=source.buffer;
                getBpm();
            } else {
                // speech recognition API not supported
                alert('speech recognition not supported');
            }
        }

        function startAudio() {
            var input = document.createElement('input');
        input.type = 'file';
        input.style = "display:none";
        input.onchange = function (e) {
            var file = e.target.files[0];
            var reader = new FileReader();
            reader.onload = function () {
                console.log(this.result);
                finalBuffer=this.result;
                audioCtx.decodeAudioData(this.result, (decodedData) => {
                    audioDatas.push(decodedData);
                    finalBuffer = decodedData;
                    getBpm();
                }, (e) => {
                    alert('Sorry this browser unable to download this file... try Chrome');
                });
            }
            reader.readAsArrayBuffer(file);
        }    

        document.getElementById('loading').appendChild(input);
        input.click();
            playSegment(0, 5);
            audio.addEventListener('timeupdate', function () {
                if (segmentEnd && song.currentTime >= segmentEnd) {
                    audio.pause();
                }
                startRecording();
            }, false);
        }

        function playSegment(startTime, endTime) {
            segmentEnd = endTime;
            song.currentTime = startTime;
            song.play();
        }

        function stopAudio() {

        }

        function startRecording() {

        }

        function endRecording() {

        }

        function receivedRecognition(event) {
            console.log(event.results[0][0].transcript);
        }

        function getPeaks(data) {
            // What we're going to do here, is to divide up our audio into parts.

            // We will then identify, for each part, what the loudest sample is in that
            // part.

            // It's implied that that sample would represent the most likely 'beat'
            // within that part.

            // Each part is 0.5 seconds long - or 22,050 samples.

            // This will give us 60 'beats' - we will only take the loudest half of
            // those.

            // This will allow us to ignore breaks, and allow us to address tracks with
            // a BPM below 120.

            var partSize = 22050,
                parts = data[0].length / partSize,
                peaks = [];

            for (var i = 0; i < parts; i++) {
                var max = 0;
                for (var j = i * partSize; j < (i + 1) * partSize; j++) {
                    var volume = Math.max(Math.abs(data[0][j]), Math.abs(data[1][j]));
                    if (!max || (volume > max.volume)) {
                        max = {
                            position: j,
                            volume: volume
                        };
                    }
                }
                peaks.push(max);
            }

            // We then sort the peaks according to volume...

            peaks.sort(function (a, b) {
                return b.volume - a.volume;
            });

            // ...take the loundest half of those...

            peaks = peaks.splice(0, peaks.length * 0.5);

            // ...and re-sort it back based on position.

            peaks.sort(function (a, b) {
                return a.position - b.position;
            });

            return peaks;
        }

        function getIntervals(peaks) {

            // What we now do is get all of our peaks, and then measure the distance to
            // other peaks, to create intervals.  Then based on the distance between
            // those peaks (the distance of the intervals) we can calculate the BPM of
            // that particular interval.

            // The interval that is seen the most should have the BPM that corresponds
            // to the track itself.

            var groups = [];

            peaks.forEach(function (peak, index) {
                for (var i = 1; (index + i) < peaks.length && i < 10; i++) {
                    var group = {
                        tempo: (60 * 44100) / (peaks[index + i].position - peak.position),
                        count: 1
                    };

                    while (group.tempo < 90) {
                        group.tempo *= 2;
                    }

                    while (group.tempo > 180) {
                        group.tempo /= 2;
                    }

                    group.tempo = Math.round(group.tempo);

                    if (!(groups.some(function (interval) {
                        return (interval.tempo === group.tempo ? interval.count++ : 0);
                    }))) {
                        groups.push(group);
                    }
                }
            });
            return groups;
        }

        function getBpm() {
            if (finalBuffer != null) {
                // Create offline context
                var OfflineContext = window.OfflineAudioContext || window.webkitOfflineAudioContext;
                var offCtx = new OfflineContext(2, 30 * 44100, 44100);

                // see below for optional constructor parameters.
                var buffer = finalBuffer;
                // Create buffer source
                var source = offCtx.createBufferSource();
                source.buffer = buffer;

                // Beats, or kicks, generally occur around the 100 to 150 hz range.
                // Below this is often the bassline.  So let's focus just on that.

                // First a lowpass to remove most of the song.

                var lowpass = offCtx.createBiquadFilter();
                lowpass.type = "lowpass";
                lowpass.frequency.value = 150;
                lowpass.Q.value = 1;

                // Run the output of the source through the low pass.
                source.connect(lowpass);

                // Now a highpass to remove the bassline.
                var highpass = offCtx.createBiquadFilter();
                highpass.type = "highpass";
                highpass.frequency.value = 100;
                highpass.Q.value = 1;

                // Run the output of the lowpass through the highpass.
                lowpass.connect(highpass);

                // Run the output of the highpass through our offline context.
                highpass.connect(offCtx.destination);

                // Start the source, and render the output into the offline conext.
                source.start(0);
                offCtx.startRendering();

                offCtx.oncomplete = function (e) {
                    var buffer = e.renderedBuffer;
                    var peaks = getPeaks([buffer.getChannelData(0), buffer.getChannelData(1)]);
                    var groups = getIntervals(peaks);

                    var top = groups.sort(function (intA, intB) {
                        return intB.count - intA.count;
                    }).splice(0, 5);
                    document.getElementById('tempo').innerText = Math.round(top[0].tempo);
                }
            }
        }
    </script>
</body>

</html>